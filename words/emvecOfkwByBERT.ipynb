{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c4aa575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T21:30:33.629625Z",
     "start_time": "2023-04-21T21:30:33.623947Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81c82c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T21:30:36.598819Z",
     "start_time": "2023-04-21T21:30:34.286778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e17939ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T21:30:37.410602Z",
     "start_time": "2023-04-21T21:30:37.406900Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuation_map = dict((ord(char), None) for char in string.punctuation)  #引入标点符号，为下步去除标点做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "966d65db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T22:00:20.121942Z",
     "start_time": "2023-04-21T22:00:20.033082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the paragraph and words to generate embeddings for\n",
    "df_paragraphs = pd.read_csv('/Users/carina/Downloads/courses/final thesis/dataset/annotated text.csv')\n",
    "\n",
    "df_keywords_YAKE5 = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/keywords_YAKE_5.csv')\n",
    "df_keywords_YAKE5 = df_keywords_YAKE5.drop(df_keywords_YAKE5.columns[0],axis=1)\n",
    "\n",
    "df_keywords_YAKE10 = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/keywords_YAKE_10.csv')\n",
    "df_keywords_YAKE10 = df_keywords_YAKE10.drop(df_keywords_YAKE10.columns[0],axis=1)\n",
    "\n",
    "df_keywords_YAKE15 = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/keywords_YAKE_15.csv')\n",
    "df_keywords_YAKE15 = df_keywords_YAKE15.drop(df_keywords_YAKE15.columns[0],axis=1)\n",
    "\n",
    "#df_cleanedtext = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/annotated_cleaned.csv')\n",
    "#df_cleanedtext = df_cleanedtext.drop(df_cleanedtext.columns[0],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7fae5f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T22:00:20.923484Z",
     "start_time": "2023-04-21T22:00:20.704412Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emvecOfkw_YAKE5 = pd.DataFrame()\n",
    "df_emvecOfkw_YAKE10 = pd.DataFrame()\n",
    "df_emvecOfkw_YAKE15 = pd.DataFrame()\n",
    "#df_emvecOfallwords = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1000199f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T22:27:04.283823Z",
     "start_time": "2023-04-21T22:00:21.583857Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [02:47,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [02:48,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317it [02:56,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [04:21,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925\n",
      "21\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:15,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171it [01:50,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [03:10,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "307it [03:12,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317it [03:19,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [04:59,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919\n",
      "9\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171it [01:45,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [02:58,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317it [03:07,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [04:30,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929\n",
      "16\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "302it [02:38,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [03:57,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887\n",
      "5\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:13,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [02:49,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317it [02:57,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "386it [03:36,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "472it [04:13,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [04:16,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "15\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:14,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:30,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "289it [02:53,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [02:59,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317it [03:08,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "335it [03:20,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [04:35,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874\n",
      "12\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "heads = df_paragraphs.columns.values.tolist()\n",
    "#for i in tqdm(range(len(heads))):\n",
    "for head in heads:\n",
    "    j = 0\n",
    "    pp = 0\n",
    "    poorkeyword = 0\n",
    "    #head = heads[i]\n",
    "    paras = []\n",
    "    paras = df_paragraphs[head].tolist()\n",
    "    \n",
    "    keyword_YAKE5 = []\n",
    "    keyword_YAKE5 = df_keywords_YAKE5[head].tolist()\n",
    "    \n",
    "    words_embeddings = []\n",
    "    for paragraphs,words in tqdm(zip(paras,keyword_YAKE5)):\n",
    "        \n",
    "        #paragraphs = paragraphs.lower()\n",
    "        paragraphs = paragraphs.translate(punctuation_map)\n",
    "        # Tokenize the paragraph and convert the tokens to IDs\n",
    "        tokens = tokenizer.tokenize(paragraphs)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Convert the input IDs to a PyTorch tensor\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "        # Generate embeddings for the input IDs using the BERT model\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        words = words.strip('[').strip(']')\n",
    "        words = words.translate(punctuation_map)\n",
    "        words = words.split(' ')\n",
    "        \n",
    "        word_embeddings = []\n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            # Find the index of the first occurrence of the word in the tokens\n",
    "            if word in tokens:\n",
    "                index = tokens.index(word)\n",
    "                # Extract the corresponding embedding vector from the BERT model output\n",
    "                embedding = outputs.last_hidden_state[0][index].detach().numpy()\n",
    "                word_embeddings.append(embedding)\n",
    "            else :\n",
    "                j = j + 1\n",
    "                tem_token = tokenizer.tokenize(word)\n",
    "                tem_em = []\n",
    "                for item in tem_token:\n",
    "                    if item in tokens:\n",
    "                        index = tokens.index(item)\n",
    "                        # Extract the corresponding embedding vector from the BERT model output\n",
    "                        embedding = outputs.last_hidden_state[0][index].detach().numpy()\n",
    "                        tem_em.append(embedding)\n",
    "                    else:\n",
    "                        pp += 1\n",
    "                        #a = [0.0] * 768\n",
    "                        #tem_em.append(a)\n",
    "                if len(tem_em)!=0:\n",
    "                    tem_em = np.mean(tem_em, axis=0)\n",
    "                    word_embeddings.append(tem_em)\n",
    "                else:\n",
    "                    tem_em = [0.0] * 768\n",
    "                    word_embeddings.append(tem_em)\n",
    "                    poorkeyword += 1\n",
    "                    print('terrible1')\n",
    "        \n",
    "        words_embeddings.append(word_embeddings)\n",
    "        #print(words_embeddings)\n",
    "        #print('one done')\n",
    "    df_emvecOfkw_YAKE5[head] = words_embeddings\n",
    "    print(j)\n",
    "    print(pp)\n",
    "    print(poorkeyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7253beaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T19:31:38.878064Z",
     "start_time": "2023-04-20T19:29:11.306416Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emvecOfkw_YAKE10.to_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/emvec_BERT_keywords_YAKE10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c46aea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T23:31:28.414829Z",
     "start_time": "2023-04-21T23:31:28.263456Z"
    }
   },
   "outputs": [],
   "source": [
    "df_keywords_tfidf5 = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/keywords_tfidf_5.csv')\n",
    "df_keywords_tfidf5 = df_keywords_tfidf5.drop(df_keywords_tfidf5.columns[0],axis=1)\n",
    "\n",
    "df_keywords_tfidf10 = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/keywords_tfidf_10.csv')\n",
    "df_keywords_tfidf10 = df_keywords_tfidf10.drop(df_keywords_tfidf10.columns[0],axis=1)\n",
    "\n",
    "df_keywords_tfidf15 = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/keywords_tfidf_15.csv')\n",
    "df_keywords_tfidf15 = df_keywords_tfidf15.drop(df_keywords_tfidf15.columns[0],axis=1)\n",
    "\n",
    "df_cleanedtext = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/annotated_cleaned.csv')\n",
    "df_cleanedtext = df_cleanedtext.drop(df_cleanedtext.columns[0],axis=1)\n",
    "\n",
    "df_origintext = pd.read_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/annotated_originalwords.csv')\n",
    "df_origintext = df_origintext.drop(df_origintext.columns[0],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "14d2f00e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T23:31:29.145916Z",
     "start_time": "2023-04-21T23:31:29.141957Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emvecOfkw_tfidf5 = pd.DataFrame()\n",
    "df_emvecOfkw_tfidf10 = pd.DataFrame()\n",
    "df_emvecOfkw_tfidf15 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25886f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T23:31:29.863564Z",
     "start_time": "2023-04-21T23:31:29.859482Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_index(lst=None, item=''):\n",
    "    return [i for i in range(len(lst)) if lst[i] == item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "95dad0a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T23:55:56.390387Z",
     "start_time": "2023-04-21T23:31:30.670815Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▉                                          | 21/480 [00:11<03:52,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible0\n",
      "terrible0\n",
      "terrible1\n",
      "terrible0\n",
      "terrible0\n",
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████                    | 258/480 [02:31<01:37,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 480/480 [04:25<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3073\n",
      "16\n",
      "7\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▉                                          | 21/480 [00:11<04:06,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible0\n",
      "terrible0\n",
      "terrible1\n",
      "terrible0\n",
      "terrible0\n",
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████                    | 258/480 [02:36<01:39,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 480/480 [04:37<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3176\n",
      "14\n",
      "7\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▉                                          | 21/480 [00:09<03:23,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible0\n",
      "terrible0\n",
      "terrible1\n",
      "terrible0\n",
      "terrible0\n",
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████                    | 258/480 [02:18<01:29,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 480/480 [04:03<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068\n",
      "16\n",
      "7\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 480/480 [03:28<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3028\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████                    | 258/480 [02:04<01:25,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 480/480 [03:41<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3042\n",
      "2\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████                    | 258/480 [02:20<01:33,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible1\n",
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████████▏                | 293/480 [02:39<01:38,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible0\n",
      "terrible0\n",
      "terrible1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 480/480 [04:08<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3103\n",
      "9\n",
      "4\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "heads = df_paragraphs.columns.values.tolist()\n",
    "#for i in tqdm(range(len(heads))):\n",
    "for head in heads:\n",
    "    j = 0\n",
    "    pp = 0\n",
    "    uselessoris = 0\n",
    "    uselesskws = 0\n",
    "    #head = heads[i]\n",
    "    paras = []\n",
    "    paras = df_paragraphs[head].tolist()\n",
    "    \n",
    "    keyword_tfidf10 = []\n",
    "    keyword_tfidf10 = df_keywords_tfidf10[head].tolist()\n",
    "    \n",
    "    clean_word = []\n",
    "    clean_word = df_cleanedtext[head].tolist()\n",
    "    \n",
    "    original_word = []\n",
    "    original_word = df_origintext[head].tolist()\n",
    "    \n",
    "    words_embeddings = []\n",
    "    #for paragraphs,words in tqdm(zip(paras,keyword_tfidf10)):\n",
    "    for i in tqdm(range(len(paras))):\n",
    "        \n",
    "        paragraphs = paras[i]\n",
    "        words = keyword_tfidf10[i]\n",
    "        cleanword = clean_word[i].strip('[').strip(']')\n",
    "        cleanword = cleanword.translate(punctuation_map)\n",
    "        cleanword = cleanword.split(' ')\n",
    "        #print(len(cleanword))\n",
    "        \n",
    "        originalword = original_word[i].strip('[').strip(']')\n",
    "        originalword = originalword.translate(punctuation_map)\n",
    "        originalword = originalword.split(' ')\n",
    "        #print(len(originalword))\n",
    "        \n",
    "        #paragraphs = paragraphs.lower()\n",
    "        paragraphs = paragraphs.translate(punctuation_map)\n",
    "        # Tokenize the paragraph and convert the tokens to IDs\n",
    "        tokens = tokenizer.tokenize(paragraphs)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Convert the input IDs to a PyTorch tensor\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "        # Generate embeddings for the input IDs using the BERT model\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        words = words.strip('[').strip(']')\n",
    "        words = words.translate(punctuation_map)\n",
    "        words = words.split(' ')\n",
    "        \n",
    "        word_embeddings = []\n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            # Find the index of the first occurrence of the word in the tokens\n",
    "            if word in tokens:\n",
    "                index = tokens.index(word)\n",
    "                # Extract the corresponding embedding vector from the BERT model output\n",
    "                embedding = outputs.last_hidden_state[0][index].detach().numpy()\n",
    "                word_embeddings.append(embedding)\n",
    "            else :\n",
    "                j += 1\n",
    "                #print(word)\n",
    "                index = get_all_index(cleanword,word)\n",
    "                #print(index)\n",
    "                origins = []\n",
    "                for ind in index:\n",
    "                    origins.append(originalword[ind])\n",
    "                #print(origins)    \n",
    "                originem = []\n",
    "                for ori in origins:\n",
    "                    tem_token = tokenizer.tokenize(ori)\n",
    "                    tem_em = []\n",
    "                    for item in tem_token:\n",
    "                        if item in tokens:\n",
    "                            #print('found')\n",
    "                            index1 = tokens.index(item)\n",
    "                            # Extract the corresponding embedding vector from the BERT model output\n",
    "                            embedding = outputs.last_hidden_state[0][index1].detach().numpy()\n",
    "                            tem_em.append(embedding)\n",
    "                        else:\n",
    "                            #print('not found')\n",
    "                            #print(ori)\n",
    "                            #print(item)\n",
    "                            pp += 1\n",
    "                            #a = [0.0] * 768\n",
    "                            #tem_em.append(a)\n",
    "                            \n",
    "                    if len(tem_em) != 0 :\n",
    "                        tem_em = np.mean(tem_em, axis=0)\n",
    "                        originem.append(tem_em)\n",
    "                    else : \n",
    "                        uselessoris += 1\n",
    "                        print('terrible0')\n",
    "                \n",
    "                if len(originem) != 0 :\n",
    "                    \n",
    "                    originem = np.mean(originem, axis=0)\n",
    "                    word_embeddings.append(originem)\n",
    "                else:\n",
    "                    uselesskws += 1\n",
    "                    originem = [0.0]*768\n",
    "                    word_embeddings.append(originem)\n",
    "                    print('terrible1')\n",
    "\n",
    "        words_embeddings.append(word_embeddings)\n",
    "        #print(words_embeddings)\n",
    "        #print('one done')\n",
    "    df_emvecOfkw_tfidf10[head] = words_embeddings\n",
    "    print(j)\n",
    "    print(pp)\n",
    "    print(uselessoris)\n",
    "    print(uselesskws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5008c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T17:54:16.590055Z",
     "start_time": "2023-04-20T17:53:05.510015Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emvecOfkw_tfidf5.to_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/emvec_BERT_keywords_tfidf5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f674c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
