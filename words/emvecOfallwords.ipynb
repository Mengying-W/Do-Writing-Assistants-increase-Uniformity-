{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "587f43d3",
   "metadata": {},
   "source": [
    "# get embedding vectors by GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405c5b2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T22:29:48.718426Z",
     "start_time": "2023-04-29T22:29:46.881320Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902595b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T22:29:52.926687Z",
     "start_time": "2023-04-29T22:29:50.300369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load GPT2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7c17e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T22:29:53.796617Z",
     "start_time": "2023-04-29T22:29:53.793114Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuation_map = dict((ord(char), None) for char in string.punctuation)  #引入标点符号，为下步去除标点做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5cb166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T22:29:54.701147Z",
     "start_time": "2023-04-29T22:29:54.644572Z"
    }
   },
   "outputs": [],
   "source": [
    "df_paragraphs = pd.read_csv('/Users/carina/Downloads/courses/final thesis/dataset/annotated text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8112d421",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T22:55:43.252214Z",
     "start_time": "2023-04-29T22:29:59.847346Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 480/480 [04:43<00:00,  1.69it/s]\n",
      "100%|█████████████████████████████████████████| 480/480 [04:52<00:00,  1.64it/s]\n",
      "100%|█████████████████████████████████████████| 480/480 [04:12<00:00,  1.90it/s]\n",
      "100%|█████████████████████████████████████████| 480/480 [03:35<00:00,  2.22it/s]\n",
      "100%|█████████████████████████████████████████| 480/480 [03:55<00:00,  2.04it/s]\n",
      "100%|█████████████████████████████████████████| 480/480 [04:23<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "heads = df_paragraphs.columns.values.tolist()\n",
    "df_emvecOfaw = pd.DataFrame()\n",
    "for head in heads:\n",
    "    paras = []\n",
    "    paras = df_paragraphs[head].tolist()\n",
    "    \n",
    "    words_embeddings = []\n",
    "    for i in tqdm(range(len(paras))):\n",
    "        \n",
    "        paragraphs = paras[i]\n",
    "        \n",
    "        paragraphs = paragraphs.lower()\n",
    "        paragraphs = paragraphs.translate(punctuation_map)\n",
    "        \n",
    "        # Tokenize the paragraph and convert the tokens to IDs\n",
    "        input_ids = torch.tensor(tokenizer.encode(paragraphs, add_special_tokens=True)).unsqueeze(0)\n",
    "        tokens = [token.replace('Ġ', '') for token in tokenizer.convert_ids_to_tokens(input_ids[0])]\n",
    "        #print('num of tokens : ')\n",
    "        #print(len(tokens))\n",
    "        \n",
    "\n",
    "        # Generate embeddings for the input IDs using the BERT model\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Get the hidden states of the last layer from the model\n",
    "        if len(outputs) >= 3:\n",
    "            last_layer_hidden_states = outputs[2][-1]\n",
    "        else:\n",
    "            last_layer_hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        word_embeddings = []\n",
    "        for token in tokens:\n",
    "                index = tokens.index(token)\n",
    "                # Extract the corresponding embedding vector from the GPT model output\n",
    "                embedding = last_layer_hidden_states[0][index].detach().numpy()\n",
    "                #print(len(embedding))\n",
    "                #print(embedding)\n",
    "                word_embeddings.append(embedding)\n",
    "                \n",
    "        words_embeddings.append(word_embeddings)\n",
    "    df_emvecOfaw[head] = words_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d52ff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T23:41:40.789870Z",
     "start_time": "2023-04-29T22:55:43.256405Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emvecOfaw.to_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/emvec_GPT2_allwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc15df",
   "metadata": {},
   "source": [
    "# get embedding vectors by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac122cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_map = dict((ord(char), None) for char in string.punctuation)  #引入标点符号，为下步去除标点做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paragraph and words to generate embeddings for\n",
    "df_paragraphs = pd.read_csv('/Users/carina/Downloads/courses/final thesis/dataset/annotated text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = df_paragraphs.columns.values.tolist()\n",
    "df_emvecOfaw = pd.DataFrame()\n",
    "#for i in tqdm(range(len(heads))):\n",
    "for head in heads:\n",
    "    paras = []\n",
    "    paras = df_paragraphs[head].tolist()\n",
    "    \n",
    "    words_embeddings = []\n",
    "    for i in tqdm(range(len(paras))):\n",
    "        \n",
    "        paragraphs = paras[i]\n",
    "        \n",
    "        #paragraphs = paragraphs.lower()\n",
    "        paragraphs = paragraphs.translate(punctuation_map)\n",
    "        \n",
    "        # Tokenize the paragraph and convert the tokens to IDs\n",
    "        tokens = tokenizer.tokenize(paragraphs)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        #print('num of tokens : ')\n",
    "        #print(len(tokens))\n",
    "        \n",
    "        # Convert the input IDs to a PyTorch tensor\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "        # Generate embeddings for the input IDs using the BERT model\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        # Generate embeddings for the input IDs using the BERT model\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        word_embeddings = []\n",
    "        for token in tokens:\n",
    "                index = tokens.index(token)\n",
    "                # Extract the corresponding embedding vector from the BERT model output\n",
    "                embedding = outputs.last_hidden_state[0][index].detach().numpy()\n",
    "                word_embeddings.append(embedding)\n",
    "        \n",
    "        words_embeddings.append(word_embeddings)\n",
    "    df_emvecOfaw[head] = words_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emvecOfaw.to_csv('/Users/carina/Downloads/courses/final thesis/precessed data/ex2/emvec_BERT_allwords.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
